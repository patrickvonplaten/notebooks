{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reformer 4/4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+1sgDFnZcLUNTEYhp1sQG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ba1341e2776c4c77ba3ef1cb678ca276": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6162dacc847c429ca2f27ac99a7b560e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5eaee701f0bb46e0910048a0556a692a",
              "IPY_MODEL_1a21f7fbff5541369250106bdcc8f1cd"
            ]
          }
        },
        "6162dacc847c429ca2f27ac99a7b560e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5eaee701f0bb46e0910048a0556a692a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bbd58baaef314653b0a3b574f2a8fc51",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1151,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1151,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f6063125c6584f239a1679c5478ccb68"
          }
        },
        "1a21f7fbff5541369250106bdcc8f1cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_611dc661229f4675abba110d179bbf98",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.15k/1.15k [00:02&lt;00:00, 546B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_53d755610bb1438a84855b5764c91fc5"
          }
        },
        "bbd58baaef314653b0a3b574f2a8fc51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f6063125c6584f239a1679c5478ccb68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "611dc661229f4675abba110d179bbf98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "53d755610bb1438a84855b5764c91fc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Reformer_4_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJOlM8rEC2SA",
        "colab_type": "text"
      },
      "source": [
        "# **The Reformer - Pushing the limits of language modeling**\n",
        "\n",
        "***How the Reformer uses less than 8GB of RAM to train on sequences of half a million tokens***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk1ETLlfEMGA",
        "colab_type": "text"
      },
      "source": [
        "The Reformer model as introduced by [Kitaev, Kaiser et al. (2020)](https://arxiv.org/pdf/2001.04451.pdf) is one of the most memory-efficient transformer models for long sequence modeling as of today.\n",
        "\n",
        "Recently, long sequence modeling has experienced a surge of interest as can be seen by the many submissions from this year alone - [Beltagy et al. (2020)](https://arxiv.org/abs/2004.05150), [Roy et al. (2020)](https://arxiv.org/abs/2003.05997), [Tay et al.](https://arxiv.org/abs/2002.11296), [Wang et al.](https://arxiv.org/abs/2006.04768) to name  a few. \n",
        "The motivation behind long sequence modeling is that many tasks in NLP, *e.g.* summarization, question answering, require the model to process longer input sequences than models, such as BERT, are able to handle. In tasks that require the model to process a large input sequence, long sequence models do not have to cut the input sequence to avoid memory overflow and thus have been shown to outperform standard \"BERT\"-like models *cf.* [Beltagy et al. (2020)](https://arxiv.org/abs/2004.05150). \n",
        "\n",
        "The Reformer pushes the limit of longe sequence modeling by its ability to process up to half a million tokens at once as shown in this [demo](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb). As a comparison, a conventional `bert-base-uncased` model limits the input length to only 512 tokens. In Reformer, each part of the standard transformer architecture is re-engineered to optimize for minimal memory requirement without a significant drop in performance.\n",
        "\n",
        "The memory improvements can be attributed to **4** features which the Reformer authors introduced to the transformer world:\n",
        "\n",
        "1.   **Reformer Self-Attention Layer** - *How to efficiently implement self-attention without being restricted to a local context?* => see [this colab](https://colab.research.google.com/drive/15oP52_7W5dRcAnbgX3tYADsu4R3cjMIf?usp=sharing)\n",
        "2.  **Chunked Feed Forward Layers** - *How to get a better time-memory trade-off for large feed forward layers?* => see [this colab](https://colab.research.google.com/drive/1xKK32Yhda-iYgtoA3eCrnCVuy_lraQR9?usp=sharing)\n",
        "3.   **Reversible Residual Layers**  - *How to drastically reduce memory consumption in training by a smart residual architecture?* => see [this colab](https://colab.research.google.com/drive/1BLffcRt9LXmM7nKU2UXhtm0PqAG0UE7J#scrollTo=mk1ETLlfEMGA)\n",
        "4.   **Axial Positional Encodings** - *How to make positional encodings usable for extremely large input sequences?*\n",
        "\n",
        "The goal of this blog post is to give the reader an **in-depth** understanding of each of the four Reformer features mentioned above. While the explanations are focussed on the Reformer, the reader should get a better intuition under which circumstances each of the four features can be effective for other transformer models as well. \n",
        "The four sections are only loosely connected, so they can very well be read individually.\n",
        "\n",
        "Reformer is part of the ðŸ¤—Transformers library. For all users of the Reformer, it is advised to go through this very detailed blog post to better understand how the model works and how to correctly set its configuration. All equations are accompanied by their equivalent name for the Reformer config, *e.g.* `config.<param_name>`, so that the reader can quickly relate to the official docs and configuration file.\n",
        "\n",
        "**Note**: *Axial Positional Encodings* are not explained in the official Reformer paper, but are extensively used in the official codebase. This blog post gives the first in-depth explanation of Axial Positional Encodings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9ox4Pm8Dgil",
        "colab_type": "text"
      },
      "source": [
        "## **4. Axial Positional Encodings**\n",
        "\n",
        "Reformer makes it possible to process huge input sequences. However, for such long input sequences standard positional encoding weight matrices alone would use more than 1GB to store its weights.\n",
        "To prevent such large positional encoding matrices, the official Reformer code makes use of *Axial Position Encodings*.\n",
        "\n",
        "**Important:** *Axial Position Encodings were not explained in the official paper, but can be well understood from looking into the code and talking to the authors*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogDIps0Zjljs",
        "colab_type": "text"
      },
      "source": [
        "### **Axial Positional Encodings in Reformer**\n",
        "\n",
        "Transformers need positional encodings to account for the order of words in the input because self-attention layers have *no notion of order*. \n",
        "Positional encodings are usually defined by a simple look-up matrix $\\mathbf{E} = \\left[\\mathbf{e}_1, \\ldots, \\mathbf{e}_{n_\\text{max}}\\right]$ The positional encoding vector $\\mathbf{e}_i$ is then simply added to the *ith* input vector $\\mathbf{x}_i + \\mathbf{e}_i$ so that the model can distinguish if an input vector (*a.k.a* token) is at position $i$ or $j$. \n",
        "For every input position, the model needs to be able to look up the corresponding positional encoding vector so that the dimension of $\\mathbf{E}$ is defined by the maximum length of input vectors the model can process `config.max_position_embeddings`, *i.e.* $n_\\text{max}$, and the `config.hidden_size`, *i.e.* $d_h$ of the input vectors. \n",
        "\n",
        "Assuming $d_h=4$ and $n_\\text{max}=49$, such a positional encoding matrix can be visualized as follows:\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/positional_encodings_default.png)\n",
        "\n",
        "Here, we showcase only the positional encodings $\\mathbf{e}_1$, $\\mathbf{e}_2$, and $\\mathbf{e}_{49}$ each of dimension, *a.k.a* height 4.\n",
        "\n",
        "Let's imagine, we want to train a Reformer model on sequences of a length of up to 0.5M tokens and an input vector `config.hidden_size` of 1024 (see notebook [here](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)). The corresponding positional embeddings have a size of $0.5M \\times 1024 \\sim 512M$ parameters, which corresponds to a size of 2GB.\n",
        "\n",
        "Such positional encodings would use an unnecessarily large amount of memory both when loading the model in memory and when saving the model on a hard drive.\n",
        "\n",
        "The Reformer authors managed to drastically shrink the positional encodings in size by cutting the `config.hidden_size` dimension in two and smartly factorizing the $n_\\text{max}$ dimension. In Transformer, the user can decide into which shape $n_\\text{max}$ can be factorized into by setting `config.axial_pos_shape` to an appropriate list of two values $n_\\text{max}^1$ and $n_\\text{max}^2$ so that $n_\\text{max}^1 \\times n_\\text{max}^2 = n_\\text{max}$. By setting `config.axial_pos_embds_dim` to an appropriate list of two values $d_h^1$ and $d_h^2$ so that $d_h^1 + d_h^2 = d_h$, the user can decide how the hidden size dimension should be cut. \n",
        "Now, let's visualize and explain more intuitively.\n",
        "\n",
        "One can think of factorizing $n_\\text{max}$ as folding the dimension into a third axis, which is shown in the following for the factorization `config.axial_pos_shape = [7, 7]`:\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/3d_positional_encoding.png)\n",
        "\n",
        "Each of the three standing rectangular prisms corresponds to one of the encoding vectors $\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_{49}$, but we can see that the 49 encoding vectors are divided into 7 rows of 7 vectors each.\n",
        "Now the idea is to use only one row of 7 encoding vectors and expand those vectors to the other 6 rows, essentially reusing their values. \n",
        "Because it is discouraged to have the same values for different encoding vectors, each vector of dimension (*a.k.a* height) `config.hidden_size=4` is cut into the lower encoding vector $\\mathbf{e}_\\text{down}$ of size $1$ and $\\mathbf{e}_\\text{up}$ of size $3$, so that the lower part can be expanded along the row dimension and the upper part can be expanded along the column dimension.\n",
        "Let's visualize for more clarity.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/3d_positional_encoding_cut.png)\n",
        "\n",
        "We can see that we have cut the embedding vectors into $\\mathbf{e}_\\text{down}$ (*in blue*) and $\\mathbf{e}_\\text{up}$ (*in yellow*).\n",
        "Now for the \"sub\"-vectors $\\mathbf{E}_\\text{down} = \\left[\\mathbf{e}_{\\text{down},1}, \\ldots, \\mathbf{e}_{\\text{down},49}\\right]$ only the first row, *a.k.a.* the width in the graphic, of $7$ is kept and expanded along the column dimension, *a.k.a.* the depth of the graphic. Inversely, for the \"sub\"-vectors $\\mathbf{E}_\\text{up} = \\left[\\mathbf{e}_{\\text{up},1}, \\ldots, \\mathbf{e}_{\\text{up},49}\\right]$ only the first column of $7$ is kept and expanded along the row dimension.\n",
        "The resulting embedding vectors $\\mathbf{e'}_i$ then correspond to\n",
        "\n",
        "  \\begin{align}\n",
        "    \\mathbf{e'}_i &= \\begin{bmatrix}\n",
        "           \\mathbf{e}_{\\text{down, } i \\% n_\\text{max}^1} \\\\\n",
        "           \\mathbf{e}_{\\text{up, } \\left \\lfloor{\\frac{i}{n_\\text{max}^2}}\\right \\rceil}\n",
        "         \\end{bmatrix}\n",
        "  \\end{align}\n",
        "whereas $n_\\text{max}^1 = 7$ and $n_\\text{max}^2 = 7$ in our example.\n",
        "These new encodings $\\mathbf{E'} = \\left[\\mathbf{e'}_1, \\ldots, \\mathbf{e'}_{n_\\text{max}}\\right]$ are called **Axial Position Encodings**. \n",
        "\n",
        "In the following, these axial position encodings are illustrated in more detail for our example.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/axial_pos_encoding.png)\n",
        "\n",
        "Now it should be more understandable how the final positional encoding vectors $\\mathbf{E'}$ are calculated only from $\\mathbf{E}_{\\text{down}}$ of dimension $d_h^1 \\times n_\\text{max}^1$ and $\\mathbf{E}_{\\text{up}}$ of dimension $d_h^2 \\times n_\\text{max}^2$.\n",
        "\n",
        "The crucial aspect to see here is that Axial Positional Encodings make sure that none of the vectors $\\left[\\mathbf{e'}_1, \\ldots, \\mathbf{e'}_{n_\\text{max}}\\right]$ are equal to each other by design and that the overall size of the encoding matrix is reduced from $n_\\text{max} \\times d_h$ to $n_\\text{max}^1 \\times d_h^1 + n_\\text{max}^2 \\times d_h^2$.\n",
        "By allowing each axial positional encoding vector to be different by design the model is given much more flexibility to learn efficient positional representations if axial positional encodings are learned by the model.\n",
        "\n",
        "To demonstrate the drastic reduction in size, \n",
        "let's assume we would have set `config.axial_pos_shape = [1024, 512]` and `config.axial_pos_embds_dim = [512, 512]` for a Reformer model that can process inputs up to a length of 0.5M tokens. The resulting axial positional encoding matrix would have had a size of only $1024 \\times 512 + 512 \\times 512 \\sim 800K$ parameters which corresponds to roughly 3MB. This is a drastic reduction from the 2GB a standard positional encoding matrix would require in this case.\n",
        "\n",
        "For a more condensed and math-heavy explanation please refer to the ðŸ¤—Transformers docs [here](https://huggingface.co/transformers/model_doc/reformer.html#axial-positional-encodings)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zo9ZLuNg8dsz"
      },
      "source": [
        "### **Benchmark**\n",
        "\n",
        "Lastly, let's also compare the peak memory consumption of conventional positional embeddings to *axial positional embeddings*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "b5whCRc_8ds7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "d152a5f5-e1ac-4663-e233-f603f6facf68"
      },
      "source": [
        "#@title Installs and Imports\n",
        "# pip installs\n",
        "!pip -qq install git+https://github.com/huggingface/transformers.git\n",
        "!pip install -qq py3nvml\n",
        "\n",
        "from transformers import ReformerConfig, PyTorchBenchmark, PyTorchBenchmarkArguments, ReformerModel"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 7.2MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 32.1MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 42.5MB/s \n",
            "\u001b[?25h  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 3.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a1xiDAWV8dtK"
      },
      "source": [
        "Positional embeddings depend only on two configuration parameters: The maximum allowed length of input sequences `config.max_position_embeddings` and `config.hidden_size`. Let's use a model that pushes the maximum allowed length of input sequences to half a million tokens, called `google/reformer-crime-and-punishment`, to see the effect of using axial positional embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVw3z_12XVS3",
        "colab_type": "text"
      },
      "source": [
        "To begin with, we will compare the shape of axial position encodings with standard positional encodings and the number of parameters in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mLMgZt_38dtR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418,
          "referenced_widgets": [
            "ba1341e2776c4c77ba3ef1cb678ca276",
            "6162dacc847c429ca2f27ac99a7b560e",
            "5eaee701f0bb46e0910048a0556a692a",
            "1a21f7fbff5541369250106bdcc8f1cd",
            "bbd58baaef314653b0a3b574f2a8fc51",
            "f6063125c6584f239a1679c5478ccb68",
            "611dc661229f4675abba110d179bbf98",
            "53d755610bb1438a84855b5764c91fc5"
          ]
        },
        "outputId": "27818a5e-6742-4278-e16e-dcd456df9d2d"
      },
      "source": [
        "config_no_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=False)  # disable axial positional embeddings\n",
        "config_pos_axial_embeds = ReformerConfig.from_pretrained(\"google/reformer-crime-and-punishment\", axial_pos_embds=True, axial_pos_embds_dim=(64, 192), axial_pos_shape=(512, 1024))  # enable axial positional embeddings\n",
        "\n",
        "print(\"Default Positional Encodings\")\n",
        "print(20 * '-')\n",
        "model = ReformerModel(config_no_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(20 * '-' + '\\n\\n')\n",
        "\n",
        "print(\"Axial Positional Encodings\")\n",
        "print(20 * '-')\n",
        "model = ReformerModel(config_pos_axial_embeds)\n",
        "print(f\"Positional embeddings shape: {model.embeddings.position_embeddings}\")\n",
        "print(f\"Num parameters of model: {model.num_parameters()}\")\n",
        "print(20 * '-' + '\\n\\n')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba1341e2776c4c77ba3ef1cb678ca276",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1151.0, style=ProgressStyle(descriptionâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Default Positional Encodings\n",
            "--------------------\n",
            "Positional embeddings shape: PositionEmbeddings(\n",
            "  (embedding): Embedding(524288, 256)\n",
            ")\n",
            "Num parameters of model: 136572416\n",
            "--------------------\n",
            "\n",
            "\n",
            "Axial Positional Encodings\n",
            "--------------------\n",
            "Positional embeddings shape: AxialPositionEmbeddings(\n",
            "  (weights): ParameterList(\n",
            "      (0): Parameter containing: [torch.FloatTensor of size 512x1x64]\n",
            "      (1): Parameter containing: [torch.FloatTensor of size 1x1024x192]\n",
            "  )\n",
            ")\n",
            "Num parameters of model: 2584064\n",
            "--------------------\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlMqQlINYbAj",
        "colab_type": "text"
      },
      "source": [
        "Having read the theory, the shape of the axial positional encoding weights should not be a surprise to the reader.\n",
        "\n",
        "Regarding the results, it can be seen that for models being capable of processing such long input sequences, it is not practical to use default positional encodings. \n",
        "In the case of `google/reformer-crime-and-punishment`, standard positional encodings alone contain more than 100M parameters. \n",
        "Axial positional encodings reduce this number to just over 200K.\n",
        "\n",
        "Lastly, let's also compare the required memory at inference time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0sh2a96d8dtL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "5d33e247-b018-408f-f2c4-455a1c81d8db"
      },
      "source": [
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512], batch_sizes=[8], models=[\"Reformer-No-Axial-Pos-Embeddings\", \"Reformer-Axial-Pos-Embeddings\"], no_speed=True, no_env_print=True)\n",
        "benchmark = PyTorchBenchmark(configs=[config_no_pos_axial_embeds, config_pos_axial_embeds], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 / 2\n",
            "2 / 2\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "Reformer-No-Axial-Pos-Embeddin       8              512             959      \n",
            "Reformer-Axial-Pos-Embeddings        8              512             447      \n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh3-zhHZZbca",
        "colab_type": "text"
      },
      "source": [
        "It can be seen that using axial positional embeddings reduces the memory requirement to approximately half in the case of `google/reformer-crime-and-punishment`."
      ]
    }
  ]
}